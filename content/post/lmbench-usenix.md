+++
date = "2016-10-26T10:34:31+08:00"
draft = true
title = "lmbench usenix"

+++

* 数据在CPU、内存、网络、文件系统和磁盘之间的传输
	* 带宽
	* 时延

lmbench与其他benchmark工具不一样的地方
------------------------------------

* IO(disk)
	* IOstone
		* 侧重于压测内存子系统的速度
	* IObench
		* 侧重于测量子系统：文件系统和磁盘的性能，这样会比较复杂且笨重
	* 另一篇论文
		* 看到很多IO性能测试历程，有不足：运行时间长、对于简单问题提出的解决方案太复杂
	* lmbench
		* lmdd
			* 测试顺序IO和随机IO，运行速度快
			* Chen和Patterson通过不同大小的数据量测试系统IO性能，而我们更偏向于在单个请求中的CPU消耗，较少关注测试中系统整体性能
* Berkeley Software Distribution's miscrobench suite
	* BSD出品一个可扩展的benchmark，用于BSD系统的回归测试，包括质量和性能。
	* 我们没有用它作为出发点(借鉴了观点)，原因是
		* 缺乏某些测试，比如内存时延
		* 有些测试太多，可能会使测试结果淹没在大量数据中
		* 采用了BSD license

* Ousterhout's Operating System benchmark
	* 提出一些测试用例
		* 测试系统调用延迟
		* 上下文切换时间
		* 文件系统性能等
	* 我们借鉴其idea作为工作基础，并在此之上进行扩展
	* 干的漂亮，待翻译。

* 网络测试
	* netperf测试网络带宽和时延，lmbench涵盖一个更小，复杂度较低的benchmark，打到同样效果
	* ttcp是应用广泛的benchmark，我们实现的版本在相同测试用例下，带宽差距小于2%，因此我们的测试用例是可信的

* McCalpin's Stream benchmark

* 总之
	* 我们搞了一套自己的，因为我们想要一个简单、可移植的benchmakr，想准确全面的测量我们认为对今天计算机系统性能的关键点。
	* 也借鉴了其他benchmark的idea。

benchmark注意事项
------------------------------

* 测试用例的大小问题
	* 比如做内存复制测试内存速度，如果太小，会被缓存，这样侧出来的数据将比数据在内存中快10倍多；然而如果过大的话，数据可能会被换页到磁盘，这样又会造成速度过慢，从而让benchmark过程显得漫长无结果。
	* lmbench采用如下两个方式解决：
		* 所有benchmakr使用循环的话都会被cache大小影响，所以循环同时增大数据量(2的倍数)直到到达最大值。这个结果可以打印出来，当benchmark不再符合cache长度的时候
		* benchmark确定系统内存足够。用一个小测试程序分配尽可能多的内存，然后清空这些内存，然后每次越过一页内存，计算每次结果。如果每次应用都需要ms级别时间，说明该页已经不在内存中了。然后测试程序从小的开始，然后干活直到有充足内存或者到达内存限制。
	
* 编译器的时间问题
	* 所有benchmark工具都是-O级别编译优化的；除了计算时钟速度和上下文切换时间时不能用优化，以产生正确结果。
	* 没有其他的优化选项，我们想从应用程序作者的角度观察性能问题。

* 多处理器相关
	* 多处理器系统在做benchmark时和单处理器一样，没有区别。有些系统允许用户将程序绑定到某个CPU上，为了更好的重用cache。
	* 我们benchmark时不会将程序绑定，因为这种方式与多处理器调度机制相悖。某些情况下，这个决定会导致一些有意思的结果。


* 计时相关
	* 时钟分辨率
		* benchmark软件计算用时是通过调用gettimeofday()获取系统时钟的，在一些系统中这个接口的分辨率是10ms，而大部分测试用例耗时10ms到千ms，所以这个接口太耗时了。为补偿这个定时器，benchmark软件一般通过多次运行来评估，一般来说就是在循环里执行，如果循环体非常快的话就用循环展开的方式多执行几遍，最后求平均。
	* 缓存
		* 如果benchmark软件期待数据在暂存缓存中，那么benchmark就多运行几次，只有最后一次记录结果就行。
		* 如果benchmakr不想测试cache性能，那就将数据大小设置的比cache大。比如，做bcopy测试时，采用复制8MB数据，这个值比当前计算机的二级缓存还大。
	* 结果可变性
		* 很多benchmark结果，比如context switch的性能测试，有一个趋势是，结果会有一定的跳动，可达30%之多。我们怀疑这是因为系统没有使用同一片物理内存，所以遇到了缓存冲突，换页等因素。所以我们多运行几次，选取最小的结果。
		* 用户做benchmark时，应该成为该系统的唯一用户。

* 使用lmbench数据库

* Bandwidth
	* Memory bandwidth
	* IPC bandwidth
	* Cached I/O bandwidth

* Latency measurements
	* Memory read latency background
	* Memory read latency
	* Operating system entry
	* Signal handling cost
	* Process creation costs
	* Context switching
	* Interprocess communication latencies
	* File system Latency
	* Disk latency


* Memory bandwidth
	* 以前评测性能时首先看MFLOPS，但是现在CPU的浮点单元并不是瓶颈，反而将数据从内存中读出来计算是瓶颈
	* 所以现在主要看的是内存的复制、读、写，通过不同大小的数据关心大规模内存传输

* Cached I/O bandwidth
	* 重用已缓存在文件系统cache的数据可能是内存瓶颈，所以我们通过 read 和 mmap两个接口来评估这类带宽，目的并不是测试磁盘IO带宽，而是测量在这一过程的CPU负载情况。
	* read接口将数据从内核文件系统缓存页中读到用户进程的内存里。传输时选择64KB作为块大小，可以将进入内核的开销降到最低。
	* bcopy和read的区别在于文件和虚拟内存系统的开销，大部分系统中，bcopy都会针对硬件有所优化，所以性能好一点。
	* read接口通过重读一个文件（一般8M大小，越过L2 缓存），以64KB为单位。每块buffer都会在用户进程中按integer相加。采用相加方式原因有两个：为了与mmap接口进行完全一致的对比，所以需要读到每一个数据；文件系统将数据写入内存的速度要比CPU读的快。我们想比较的是CPU将数据传输给应用程序的速度，而不是那种内存DMA的速度。
	* mmap接口提供了一个不需要复制数据就可以读文件缓存的方式，我们通过所有内存内容相加的方式使数据强制经过CPU路径。
	* 案例比较


* Latency measurements
	* 时延是性能测试中经常被忽略的点，可能因为解决时延问题更难一点。lmbench的测试项中包括内存时延，操作系统入口开销，信号处理开销，进程创建开销，上下文切换的开销，ipc，文件系统和磁盘的时延

* Memory read latency background 读取内存产生时延的背景
	* 从内存时延的角度我们可以理解和解释其他时延的产生原理，比如上下文切换的时延的产生，因为首先要保存当前进程状态，然后载入下一个进程。然而内存时延难以准确测量，并且经常被误解。
	* 读内存时延有诸多方面，内存芯片的时钟，处理器和内存的回路，从内存中首次加载数据，内存写回时延。

	* Memory chip cycle latency
		* 内存芯片的时钟都是纳秒级的，比如1333MHz的内存；DRAM的工作方式需要60到100ns时间，让内存数据稳定读取，这个级别是读内存时内存芯片准备好数据所必须的。
	* pin-to-pin lantency
		* 处理器和内存之间有总线，这个时间是处理器发出读操作后到达内存子系统的时延；尤其多插槽上的多根内存时，访问速度会更慢
	* laod-in-vacuum latency
		* 系统总线空闲时，从内存读取数据到成功的时延，这个经常被认为是内存时延；考虑到有些cpu是非阻塞读取数据的，读数据时总线不停顿，所以感知到的时延要比实际时延小；然而压测时，由于大量读写，实际的时延又要比它大。
	* 



* 内存延时有多种定义
	* 内存芯片的时钟
		* 内存芯片本身的时钟，比如1333MHz内存芯片
		* DRAM等待数据稳定
	* 处理器发出的读信号到达内存芯片的时间
		* 处理器发出读数据的地址，经过系统总线和内存子系统，到达内存
		* 内存往往分布在多根插槽，比单内存芯片时延要高
	* 总线空闲时读取内存数据
		* 处理器等待从内存中取数据的时间
			* 这个时间通常是一种标称值，有些处理器取数据时并不等待和停顿，因此测量延时可能显得小于标称值
			* 而压测时，由于突发读取导致cache miss，导致实际延时又比这个值大
			* 因此采用这个值也不是那么合理
	* 连续繁忙读内存数据
		* 连续读内存时，每次load都会跟着一个load。
		* 连续读可能会导致时延高于空闲读，有些系统会有"关键字优先"的机制，读取某个字时不等待整个cache line填充就把该line中的要读取数据喂给CPU，然而此时cache依然处于busy状态；如果此时再有第二次load，会因为当前cache busy而停顿等待。在UltraSPARC中空闲读和连续读的差异可达35%.



